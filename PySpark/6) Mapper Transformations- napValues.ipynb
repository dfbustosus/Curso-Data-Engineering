{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['SPARK_HOME']=r'C:/spark/'\n",
    "os.environ['HADOOP_HOME'] = r'C:/hadoop/'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='lab'\n",
    "os.environ['PYSPARK_PYTHON']='python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark datasets**\n",
    "\n",
    "1. RDD\n",
    "- Low Level API\n",
    "- Funcionales\n",
    "\n",
    "2. DataFrames\n",
    "- High Level API\n",
    "- Relacional\n",
    "- Optimizados para querys\n",
    "\n",
    "**Mapper Transformations**\n",
    "\n",
    "1. map(f) >> Relacion 1 a 1 >> aplica la funcion f() a un RDD\n",
    "2. mapValues(f) >> Relacion 1 a 1 >> pasa cada pareja (key,value) del RDD a f()\n",
    "3. flatMap(f) >> Relacion 1 a muchos >> Aplica la funcion f() a los elementos del RDD y aplana los resultados\n",
    "4. flatMapValues(f) >> Relacion 1 a muchos >> Pasa cada valor (key,value) del RDD por el flatMap(f) sin cambiar keys\n",
    "5. mapPartitions(f) >> Relacion muchos a 1 >> Devuelve un RDD aplicando la funcion f() a cada particion del RDD fuente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. mapValues()\n",
    "\n",
    "\n",
    "- Es especifica para key-value pairs en RDDs.\n",
    "- Aplica la funcion a cada uno de los valores menteniendo los keys sin cambio.\n",
    "- Produce un nuevo key-value RDD con los datos transformados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', []), ('Z', [40]), ('C', [10, 20, 30]), ('D', [60, 70])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"App1\").getOrCreate()\n",
    "# Data\n",
    "data = [\n",
    "    (\"A\", []), (\"Z\", [40]),\n",
    "    (\"C\", [10, 20, 30]), (\"D\", [60, 70])\n",
    "]\n",
    "# Creamos el RDD\n",
    "rdd = spark.sparkContext.parallelize(data) # RDD[integer]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 0), ('Z', 2), ('C', 4), ('D', 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    if len(x) == 0: return 0\n",
    "    else: return len(x)+1\n",
    "rdd2 = rdd.mapValues(f)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>App1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d432a79ca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (2, 6), (3, 6)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otro ejemplo (Funciones Lambda)\n",
    "rdd = spark.sparkContext.parallelize([(1, 'apple'), (2, 'banana'), (3, 'cherry')])\n",
    "result = rdd.mapValues(lambda x: len(x))\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. flatMap\n",
    "- Es una transformacion one-to-many\n",
    "- Toma un elemento del RDD y lo transforma en muchos (0,1,2,3, etc)\n",
    "- No funciona con el Spark Dataframe, pero si tiene la funcion explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------+\n",
      "|nombre|Lenguajes            |\n",
      "+------+---------------------+\n",
      "|David |[Java, Scala, Python]|\n",
      "|Juan  |[Cobol, C]           |\n",
      "|bob   |[C++]                |\n",
      "|ted   |[]                   |\n",
      "|andres|[]                   |\n",
      "+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data= [\n",
    "    ('David',['Java','Scala','Python']),\n",
    "    ('Juan',['Cobol','C']),\n",
    "    ('bob',['C++']),\n",
    "    ('ted',[]),\n",
    "    ('andres',[])\n",
    "]\n",
    "# Crear dataframe\n",
    "df = spark.createDataFrame(\n",
    "    data= data, schema= ['nombre','Lenguajes']\n",
    ")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|nombre|Lang  |\n",
      "+------+------+\n",
      "|David |Java  |\n",
      "|David |Scala |\n",
      "|David |Python|\n",
      "|Juan  |Cobol |\n",
      "|Juan  |C     |\n",
      "|bob   |C++   |\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "# explode\n",
    "exploded=df.select(df.nombre,\n",
    "          explode(df.Lenguajes).alias('Lang'))\n",
    "exploded.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------+\n",
      "|nombre|           lenguajes|    educacion|\n",
      "+------+--------------------+-------------+\n",
      "| david|[Java, Scala, Pyt...|    [MS, PHD]|\n",
      "|andrea|     [Cobol, Snobol]|     [BS, MS]|\n",
      "| pedro|               [C++]|[BS, MS, PHD]|\n",
      "|  juan|                  []|     [BS, MS]|\n",
      "|andres|           [FORTRAN]|           []|\n",
      "| sofia|                  []|           []|\n",
      "+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Si quiero para varias columnas\n",
    "some_data = [\n",
    "    ('david', ['Java','Scala', 'Python'], ['MS', 'PHD']),\n",
    "    ('andrea', ['Cobol','Snobol'], ['BS', 'MS']),\n",
    "    ('pedro', ['C++'], ['BS', 'MS', 'PHD']),\n",
    "    ('juan', [], ['BS', 'MS']),\n",
    "    ('andres', ['FORTRAN'], []),\n",
    "    ('sofia', [], [])\n",
    "]\n",
    "\n",
    "df= spark.createDataFrame(data=some_data,\n",
    "                          schema=['nombre','lenguajes','educacion'])\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------+\n",
      "|nombre|leng   |educacion    |\n",
      "+------+-------+-------------+\n",
      "|david |Java   |[MS, PHD]    |\n",
      "|david |Scala  |[MS, PHD]    |\n",
      "|david |Python |[MS, PHD]    |\n",
      "|andrea|Cobol  |[BS, MS]     |\n",
      "|andrea|Snobol |[BS, MS]     |\n",
      "|pedro |C++    |[BS, MS, PHD]|\n",
      "|andres|FORTRAN|[]           |\n",
      "+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_1= df.select(\n",
    "    df.nombre,\n",
    "    explode(df.lenguajes).alias('leng'),\n",
    "    df.educacion\n",
    ")\n",
    "exploded_1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|nombre|leng  |grado|\n",
      "+------+------+-----+\n",
      "|david |Java  |MS   |\n",
      "|david |Java  |PHD  |\n",
      "|david |Scala |MS   |\n",
      "|david |Scala |PHD  |\n",
      "|david |Python|MS   |\n",
      "|david |Python|PHD  |\n",
      "|andrea|Cobol |BS   |\n",
      "|andrea|Cobol |MS   |\n",
      "|andrea|Snobol|BS   |\n",
      "|andrea|Snobol|MS   |\n",
      "|pedro |C++   |BS   |\n",
      "|pedro |C++   |MS   |\n",
      "|pedro |C++   |PHD  |\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_2= exploded_1.select(\n",
    "    exploded_1.nombre,\n",
    "    exploded_1.leng,\n",
    "    explode(exploded_1.educacion).alias('grado')\n",
    ")\n",
    "exploded_2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo sobre datos reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_1: int, id_2: int, cmp_fname_c1: double, cmp_fname_c2: double, cmp_lname_c1: double, cmp_lname_c2: double, cmp_sex: int, cmp_bd: int, cmp_bm: int, cmp_by: int, cmp_plz: int, is_match: boolean]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Crear una instancia de  SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "parse= spark.read.option(\"header\", \"true\").option(\"nullValue\",\"?\").\\\n",
    "    option(\"inferSchema\",\"true\").csv(\"./prueba/block_1/block_1.csv\")\n",
    "parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|37291|53113|0.833333333333333|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "|39086|47614|              1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|70031|70237|              1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parse.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a rdd\n",
    "rdd = parse.rdd\n",
    "flat_mapped_rdd = rdd.flatMap(lambda row: [(i, row[i]) for i in range(len(row))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 37291), (1, 53113)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_mapped_rdd.collect()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 194.40877763581852), (1, 231.76257830719504)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def sqrt_value(value):\n",
    "    if value is not None:\n",
    "        return math.sqrt(value)+1.3\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "mapped_rdd = flat_mapped_rdd.mapValues(sqrt_value)\n",
    "\n",
    "mapped_rdd.collect()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
